import json
from concurrent.futures import ThreadPoolExecutor, as_completed
import requests
from bs4 import BeautifulSoup
from crawler import crawl_site
from xss_payload_loader import load_xss_payloads
from scanner import scan_xss
from sqli_scanner import scan_sqli
from sqli_payload_loader import load_sqli_payloads
from report_generator import generate_html_report  # âœ… Zaten var

# Ayarlar
MAX_WORKERS = 8
TIMEOUT = 30
RESULT_FILE = "reports/scan_results.json"

session = requests.Session()
session.headers.update({"User-Agent": "VulnScanner/2.0"})

def scan_url_for_forms(url, payloads):
    """Bir URL'deki formlarÄ± bulur ve test eder."""
    try:
        res = session.get(url, timeout=TIMEOUT)
    except Exception as e:
        return {"url": url, "error": f"GET error: {e}"}

    soup = BeautifulSoup(res.text, "html.parser")
    forms = soup.find_all("form")

    findings = []
    for form in forms:
        inputs = [inp.get("name") for inp in form.find_all("input") if inp.get("name")]
        details = {
            "action": form.get("action") or url,
            "method": form.get("method", "get").lower(),
            "inputs": inputs
        }
        found = scan_xss(details, url, payloads)
        if found:
            findings.append({"form": details, "payloads": found})
    return {"url": url, "xss": findings}

def scan_url_for_sqli(url):
    """SQLi iÃ§in tarama yapar."""
    try:
        res = scan_sqli(url)
        return {"url": url, "sqli": res if res else False}
    except Exception as e:
        return {"url": url, "sqli_error": str(e)}

def worker_scan(url, payloads):
    """URL iÃ§in hem XSS hem SQLi taramasÄ± yapar."""
    xss_res = scan_url_for_forms(url, payloads)
    sqli_res = scan_url_for_sqli(url)

    combined = {"url": url}
    combined.update(xss_res)
    combined.update(sqli_res)
    return combined

def run(start_url):
    print("[*] BaÅŸlatÄ±lÄ±yor, crawler Ã§alÄ±ÅŸÄ±yor...")
    urls = crawl_site(start_url)

    # PayloadlarÄ± yÃ¼kle
    xss_payloads = load_xss_payloads("payloads/xss.txt")

    results = []
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
        futures = {ex.submit(worker_scan, u, xss_payloads): u for u in urls}
        for fut in as_completed(futures):
            u = futures[fut]
            try:
                r = fut.result()
                results.append(r)

                # Konsola Ã¶zet yazdÄ±r
                print(f"\n--- {u} ---")
                if r.get("xss"):
                    print("[XSS] Forms with payloads:")
                    for f in r["xss"]:
                        print("  form:", f["form"])
                        for p in f["payloads"]:
                            print("    -", p)
                if r.get("sqli"):
                    print("[SQLi] Possible SQLi found")
            except Exception as e:
                print(f"[!] Hata {u}: {e}")
    # JSON rapor kaydet
    with open(RESULT_FILE, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"[+] Tarama bitti. SonuÃ§lar: {RESULT_FILE}")

    # HTML RAPOR OLUÅžTUR (Ã–NEMLÄ°!)
    print("\n[*] HTML raporu oluÅŸturuluyor...")
    try:
        html_file = generate_html_report(results, start_url)
        print(f"[+] âœ… HTML raporu hazÄ±r: {html_file}")
        print(f"[+] ðŸ“‚ Raporu aÃ§mak iÃ§in: start {html_file}")
    except Exception as e:
        print(f"[!] HTML raporu oluÅŸturulamadÄ±: {e}")

if __name__ == "__main__":
    target = input("Hedef URL (Ã¶rn http://127.0.0.1:5000): ").strip()
    run(target)